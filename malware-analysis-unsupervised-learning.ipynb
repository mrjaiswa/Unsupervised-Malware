{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-06-17T21:46:53.904050Z",
     "iopub.status.busy": "2023-06-17T21:46:53.903271Z",
     "iopub.status.idle": "2023-06-17T21:46:53.946238Z",
     "shell.execute_reply": "2023-06-17T21:46:53.945083Z",
     "shell.execute_reply.started": "2023-06-17T21:46:53.904008Z"
    }
   },
   "source": [
    "# Malware-Detection-Unsupervised-Learning\n",
    "\n",
    "#  Project Topic\n",
    "\n",
    "In this project we want to build a unsupervised machine learning model that is able to predict whether  a piece of software is malicious or not. \n",
    "\n",
    "\n",
    "#  Project Goal\n",
    "The goal of a malware analysis project is to analyze and understand malicious software (malware) to identify its characteristics, behavior, and potential threats it poses. \n",
    "The primary objective is to uncover the inner workings of the malware, detect its presence, and develop countermeasures to protect systems from its harmful effects. Here are some other goals that this project can accomplish\n",
    "\n",
    "    Malware Detection: Develop techniques and tools to detect the presence of malware in various forms, such as files, network traffic, or system memory.\n",
    "\n",
    "    Malware Classification: Classify different types of malware based on their characteristics, behavior, or code patterns. This helps in identifying and categorizing malware samples for further analysis.\n",
    "\n",
    "    Malware Behavior Analysis: Investigate the behavior and capabilities of malware to understand its actions, such as data theft, system modifications, or communication with command-and-control servers.\n",
    "\n",
    "    Malware Reverse Engineering: Reverse engineer malware samples to understand their inner workings, including the code structure, algorithms, and techniques used. This helps in identifying vulnerabilities, developing countermeasures, or extracting indicators of compromise (IOCs).\n",
    "\n",
    "    Threat Intelligence: Gather and analyze data about malware campaigns, attack vectors, and emerging threats. This information can be used to improve detection methods, enhance security practices, and provide timely warnings to protect against new malware strains.\n",
    "\n",
    "    Incident Response and Mitigation: Develop strategies and tools for rapid incident response, containment, and remediation when malware infections occur. This involves identifying the extent of the compromise, removing malware, and restoring affected systems to a secure state.\n",
    "\n",
    "    Security Solutions Development: Putting th emodel in Security products like snort, firewall for easier analysis.\n",
    "\n",
    "    Threat Hunting: Proactively search for signs of malware or indicators of compromise within a network or system environment. This involves using various tools, techniques, and threat intelligence sources to identify and respond to potential malware incidents.\n",
    "\n",
    "Unsupervised learning techniques can be employed in the process of malware analysis to discover patterns, anomalies, and relationships within the data without relying on labeled examples. \n",
    "\n",
    "#  Dataset:\n",
    "https://ieee-dataport.org/open-access/malware-analysis-datasets-pe-section-headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Github:\n",
    "    \n",
    "https://github.com/mrjaiswa/Unsupervised-Malware\n",
    "\n",
    "\n",
    "Other Project:\n",
    "\n",
    "https://github.com/mrjaiswa/Deeplearning-Main-Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sections:\n",
    "\n",
    "\n",
    "1. Library Imports\n",
    "\n",
    "2. Describing Dataset\n",
    "\n",
    "3. EDA\n",
    "\n",
    "4. Data Visualization\n",
    "\n",
    "5. Unsupervised Learning\n",
    "    1. K-means\n",
    "    2. GMM\n",
    "    \n",
    "6. Hyperparamter Tuning\n",
    "\n",
    "7. Supervised Learning\n",
    "\n",
    "    1. Logistic Regression\n",
    "    2. Random Forest\n",
    "8. Conclusion\n",
    "\n",
    "9. References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Features in  the Set**\n",
    "\n",
    "Column name: hash\n",
    "\n",
    "        Description: MD5 hash of the example\n",
    "\n",
    "        Content: 32 bytes string\n",
    "\n",
    "\n",
    "Column name: size_of_data\n",
    "\n",
    "        Description: The size of the section on disk\n",
    "\n",
    "        Content: Integer\n",
    "\n",
    "\n",
    "Column name: virtual_address\n",
    "\n",
    "        Description: Memory address of the first byte of the section relative to the image base\n",
    "\n",
    "        Content: Integer\n",
    "\n",
    "\n",
    "Column name: entropy\n",
    "\n",
    "        Description: Calculated entropy of the section\n",
    "\n",
    "        Content: Float\n",
    "\n",
    "\n",
    "Column name: virtual_size\n",
    "\n",
    "        Description: The size of the section when loaded into memory\n",
    "\n",
    "        Content: Integer\n",
    "\n",
    "\n",
    "Column name: malware\n",
    "\n",
    "        Description: Class\n",
    "\n",
    "        Content: 0 (Goodware) or 1 (Malware)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why to use Unsupervised Learning\n",
    "\n",
    "    Discovering Hidden Patterns: Unsupervised learning allows for the identification of hidden patterns, structures, or relationships within the data that may not be apparent initially. It helps in gaining insights and understanding the data in a more exploratory manner.\n",
    "\n",
    "    No Need for Labeled Data: Unsupervised learning models do not require labeled data, which can be expensive and time-consuming to obtain. This makes it easier to work with large amounts of unlabeled data, which is often readily available.\n",
    "\n",
    "    Flexibility and Adaptability: Unsupervised learning can handle a wide range of data types and formats. It can be applied to various domains and problems, making it a flexible approach for data analysis and exploration.\n",
    "\n",
    "    Feature Extraction and Dimensionality Reduction: Unsupervised learning techniques such as clustering and dimensionality reduction can be used to extract important features or reduce the dimensionality of the data. This can help in improving the efficiency and performance of subsequent supervised learning tasks.\n",
    "\n",
    "Cons of Unsupervised Learning:\n",
    "\n",
    "    Lack of Clear Evaluation Metrics: Since unsupervised learning deals with unlabeled data, it can be challenging to evaluate the performance and accuracy of the models objectively. The absence of ground truth labels makes it difficult to assess the quality of clustering or pattern discovery.\n",
    "\n",
    "    Interpretability and Subjectivity: Unsupervised learning models often generate results that require interpretation. Interpreting and understanding the discovered patterns or clusters can be subjective and may vary depending on the domain or context.\n",
    "\n",
    "    Difficulty in Handling Noisy or Outlier Data: Unsupervised learning models can be sensitive to noisy or outlier data points, which may adversely affect the clustering or pattern identification process. Preprocessing and outlier detection techniques may be required to mitigate these issues.\n",
    "\n",
    "    Lack of Explicit Guidance: Unlike supervised learning, unsupervised learning does not have explicit guidance or target variables. This means that the model is not directly optimized for a specific task or objective. The lack of explicit guidance can make it challenging to use unsupervised learning models for specific prediction or classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models used in this project:\n",
    "\n",
    "1. K-means \n",
    "2. GMM\n",
    "\n",
    "\n",
    "K-means is an unsupervised machine learning algorithm used for clustering. It aims to partition a given dataset into K distinct non-overlapping clusters, where each data point belongs to the cluster with the nearest mean (centroid).\n",
    "\n",
    "Gaussian Mixture Models (GMM) is a probabilistic model used for clustering and density estimation. It assumes that the data points are generated from a mixture of Gaussian distributions, where each Gaussian component represents a cluster.\n",
    "\n",
    "GMM is specifically used for clustering, which is an unsupervised learning task. It assumes that the data is generated from a mixture of Gaussian distributions and seeks to estimate the parameters of these distributions to identify clusters in the data. The algorithm assigns data points to different clusters based on their likelihood of belonging to each Gaussian component."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supervised Learning I used are:\n",
    "1. KNN\n",
    "2. logistic Regression\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Python Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-18T07:04:56.203249Z",
     "iopub.status.busy": "2023-06-18T07:04:56.202825Z",
     "iopub.status.idle": "2023-06-18T07:04:56.211857Z",
     "shell.execute_reply": "2023-06-18T07:04:56.210171Z",
     "shell.execute_reply.started": "2023-06-18T07:04:56.203214Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import TSNE\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Description**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-18T06:54:12.810718Z",
     "iopub.status.busy": "2023-06-18T06:54:12.810274Z",
     "iopub.status.idle": "2023-06-18T06:54:12.974913Z",
     "shell.execute_reply": "2023-06-18T06:54:12.973337Z",
     "shell.execute_reply.started": "2023-06-18T06:54:12.810681Z"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('/kaggle/input/malware-analysis-datasets-pe-section-headers/pe_section_headers.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-18T06:54:42.235983Z",
     "iopub.status.busy": "2023-06-18T06:54:42.235561Z",
     "iopub.status.idle": "2023-06-18T06:54:42.273566Z",
     "shell.execute_reply": "2023-06-18T06:54:42.272361Z",
     "shell.execute_reply.started": "2023-06-18T06:54:42.235949Z"
    }
   },
   "outputs": [],
   "source": [
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-18T06:55:58.889487Z",
     "iopub.status.busy": "2023-06-18T06:55:58.889088Z",
     "iopub.status.idle": "2023-06-18T06:55:58.930492Z",
     "shell.execute_reply": "2023-06-18T06:55:58.929326Z",
     "shell.execute_reply.started": "2023-06-18T06:55:58.889457Z"
    }
   },
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-18T06:57:24.064047Z",
     "iopub.status.busy": "2023-06-18T06:57:24.063651Z",
     "iopub.status.idle": "2023-06-18T06:57:24.089479Z",
     "shell.execute_reply": "2023-06-18T06:57:24.088141Z",
     "shell.execute_reply.started": "2023-06-18T06:57:24.064017Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "data.isnull().sum()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-18T06:57:39.041195Z",
     "iopub.status.busy": "2023-06-18T06:57:39.040797Z",
     "iopub.status.idle": "2023-06-18T06:57:39.067628Z",
     "shell.execute_reply": "2023-06-18T06:57:39.066519Z",
     "shell.execute_reply.started": "2023-06-18T06:57:39.041166Z"
    }
   },
   "outputs": [],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-18T07:01:22.686052Z",
     "iopub.status.busy": "2023-06-18T07:01:22.685619Z",
     "iopub.status.idle": "2023-06-18T07:01:23.089454Z",
     "shell.execute_reply": "2023-06-18T07:01:23.088378Z",
     "shell.execute_reply.started": "2023-06-18T07:01:22.686022Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.scatterplot(x='virtual_address', y='size_of_data', data=data)\n",
    "plt.title('Scatter Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-18T07:08:49.314201Z",
     "iopub.status.busy": "2023-06-18T07:08:49.313797Z",
     "iopub.status.idle": "2023-06-18T07:08:50.555113Z",
     "shell.execute_reply": "2023-06-18T07:08:50.553897Z",
     "shell.execute_reply.started": "2023-06-18T07:08:49.314171Z"
    }
   },
   "outputs": [],
   "source": [
    "df.plot(kind='bar', x='size_of_data', y='virtual_address')\n",
    "plt.title('Bar Plot')\n",
    "plt.xlabel('Category')\n",
    "plt.ylabel('Value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation Matrix\n",
    "\n",
    "A correlation matrix is a table that displays the correlation coefficients between multiple variables. It is commonly used in data analysis and helps to understand the relationships and dependencies between variables in a dataset. In the context of a correlation matrix, colinearity refers to a high degree of correlation between two or more variables. When two variables are highly correlated, it indicates that they are linearly related, and changes in one variable are associated with predictable changes in the other variable. Colinearity can be detected by observing the correlation coefficients in the correlation matrix.\n",
    "\n",
    "\n",
    "Identify relationships\n",
    "\n",
    "Variable selection\n",
    "\n",
    "Detect patterns and dependencies\n",
    "\n",
    "Multivariate analysis\n",
    "\n",
    "Data preprocessing\n",
    "\n",
    "Outlier detection\n",
    "\n",
    "Visual representation\n",
    "\n",
    "\n",
    "\n",
    "While a high correlation between two variables suggests a relationship, it does not necessarily imply a cause-and-effect relationship. correlation matrices are valuable tools for exploratory data analysis, variable selection, and understanding the relationships between variables in a dataset. They provide a concise summary of the data and serve as a starting point for further analysis and decision-making.\n",
    "Colinearity in a correlation matrix can have both pros and cons:\n",
    "\n",
    "Pros:\n",
    "\n",
    "    Identification of Relationships: Correlated variables can provide insights into the relationships and associations between different variables in a dataset. They can indicate potential cause-and-effect relationships or common underlying factors.\n",
    "\n",
    "    Feature Selection: Correlated variables can help in feature selection by identifying redundant variables. If two variables are highly correlated, keeping both of them in a predictive model might not add much additional information. Removing one of the correlated variables can simplify the model and improve interpretability.\n",
    "\n",
    "Cons:\n",
    "\n",
    "    Multicollinearity: High correlation or colinearity among predictor variables in a predictive model can lead to multicollinearity. Multicollinearity can make it difficult to interpret the individual effects of predictor variables and can impact the stability and reliability of the model's coefficients or feature importance estimates.\n",
    "\n",
    "    Overfitting: In some cases, including highly correlated variables in a predictive model can lead to overfitting. Overfitting occurs when a model performs well on the training data but fails to generalize well to new, unseen data. Including correlated variables may increase the complexity of the model and make it overly sensitive to the training data, leading to poor generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-17T21:48:28.187436Z",
     "iopub.status.busy": "2023-06-17T21:48:28.186543Z",
     "iopub.status.idle": "2023-06-17T21:48:28.649838Z",
     "shell.execute_reply": "2023-06-17T21:48:28.648635Z",
     "shell.execute_reply.started": "2023-06-17T21:48:28.187392Z"
    }
   },
   "outputs": [],
   "source": [
    "print(data.describe())\n",
    "\n",
    "# Create a correlation matrix\n",
    "correlation_matrix = data.corr()\n",
    "\n",
    "# Create a heatmap of the correlation matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', square=True)\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-17T21:49:35.421428Z",
     "iopub.status.busy": "2023-06-17T21:49:35.421002Z",
     "iopub.status.idle": "2023-06-17T21:49:35.448403Z",
     "shell.execute_reply": "2023-06-17T21:49:35.447425Z",
     "shell.execute_reply.started": "2023-06-17T21:49:35.421393Z"
    }
   },
   "outputs": [],
   "source": [
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-17T21:50:00.745720Z",
     "iopub.status.busy": "2023-06-17T21:50:00.745280Z",
     "iopub.status.idle": "2023-06-17T21:50:01.060383Z",
     "shell.execute_reply": "2023-06-17T21:50:01.059158Z",
     "shell.execute_reply.started": "2023-06-17T21:50:00.745688Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "ax.hist(data['malware'],20)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying this with Different Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why I chose Kmeans:\n",
    "\n",
    "    K-means clustering can provide insights into the underlying structure of your data by grouping similar data points together. It helps in understanding patterns, relationships, and potential clusters within your dataset.\n",
    "\n",
    "    K-means is an unsupervised learning algorithm, which means it doesn't require labeled data for training. It can be useful when you have a large amount of unlabeled data and want to discover hidden patterns or groupings.\n",
    "\n",
    "    K-means is computationally efficient and can handle large datasets with a large number of features. It is relatively fast and can scale well with the number of data points.\n",
    "\n",
    "    K-means provides easily interpretable results. The centroids of the clusters represent the mean of the data points within each cluster, making it intuitive to understand the characteristics of each cluster.\n",
    "\n",
    "    K-means can be used for various clustering analysis tasks such as customer segmentation, image compression, anomaly detection, document clustering, and market segmentation. It is a versatile algorithm with applications in different domains.\n",
    "\n",
    "    K-means is straightforward to implement and is readily available in popular machine learning libraries such as scikit-learn. It has well-defined steps and parameters that are easy to understand and tune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-17T21:53:40.511635Z",
     "iopub.status.busy": "2023-06-17T21:53:40.511228Z",
     "iopub.status.idle": "2023-06-17T21:53:40.530748Z",
     "shell.execute_reply": "2023-06-17T21:53:40.529680Z",
     "shell.execute_reply.started": "2023-06-17T21:53:40.511598Z"
    }
   },
   "outputs": [],
   "source": [
    "features = ['size_of_data', 'virtual_address', 'entropy', 'virtual_size']\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(data[features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-17T21:53:51.153027Z",
     "iopub.status.busy": "2023-06-17T21:53:51.151626Z",
     "iopub.status.idle": "2023-06-17T21:53:52.207556Z",
     "shell.execute_reply": "2023-06-17T21:53:52.206670Z",
     "shell.execute_reply.started": "2023-06-17T21:53:51.152975Z"
    }
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=2, random_state=42)\n",
    "kmeans.fit(scaled_data)\n",
    "\n",
    "# Add cluster labels to the original dataset\n",
    "data['kmeans_cluster'] = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-17T21:54:09.662066Z",
     "iopub.status.busy": "2023-06-17T21:54:09.661308Z",
     "iopub.status.idle": "2023-06-17T21:54:10.908160Z",
     "shell.execute_reply": "2023-06-17T21:54:10.907224Z",
     "shell.execute_reply.started": "2023-06-17T21:54:09.662031Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.scatter(data['virtual_address'], data['entropy'], c=data['kmeans_cluster'])\n",
    "plt.xlabel('Virtual Address')\n",
    "plt.ylabel('Entropy')\n",
    "plt.title('K-means Clustering')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-17T21:54:29.123466Z",
     "iopub.status.busy": "2023-06-17T21:54:29.123071Z",
     "iopub.status.idle": "2023-06-17T21:54:31.191637Z",
     "shell.execute_reply": "2023-06-17T21:54:31.188954Z",
     "shell.execute_reply.started": "2023-06-17T21:54:29.123435Z"
    }
   },
   "outputs": [],
   "source": [
    "gmm = GaussianMixture(n_components=2, random_state=42)\n",
    "gmm.fit(scaled_data)\n",
    "\n",
    "# Add cluster labels to the original dataset\n",
    "data['gmm_cluster'] = gmm.predict(scaled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-17T21:54:42.191558Z",
     "iopub.status.busy": "2023-06-17T21:54:42.190525Z",
     "iopub.status.idle": "2023-06-17T21:54:43.477646Z",
     "shell.execute_reply": "2023-06-17T21:54:43.476611Z",
     "shell.execute_reply.started": "2023-06-17T21:54:42.191506Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.scatter(data['virtual_address'], data['entropy'], c=data['gmm_cluster'])\n",
    "plt.xlabel('Virtual Address')\n",
    "plt.ylabel('Entropy')\n",
    "plt.title('Gaussian Mixture Model Clustering')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-17T21:55:06.161075Z",
     "iopub.status.busy": "2023-06-17T21:55:06.160692Z",
     "iopub.status.idle": "2023-06-17T21:56:08.717939Z",
     "shell.execute_reply": "2023-06-17T21:56:08.716520Z",
     "shell.execute_reply.started": "2023-06-17T21:55:06.161049Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Parameter grid for K-means clustering\n",
    "kmeans_params = {\n",
    "    'n_clusters': [2, 3, 4, 5],  # Vary the number of clusters\n",
    "    'random_state': [42]\n",
    "}\n",
    "\n",
    "# Parameter grid for Gaussian Mixture Model (GMM)\n",
    "gmm_params = {\n",
    "    'n_components': [2, 3, 4, 5],  # Vary the number of components\n",
    "    'random_state': [42]\n",
    "}\n",
    "\n",
    "# Perform grid search for K-means clustering\n",
    "kmeans_grid = GridSearchCV(KMeans(), kmeans_params)\n",
    "kmeans_grid.fit(scaled_data)\n",
    "\n",
    "# Perform grid search for GMM\n",
    "gmm_grid = GridSearchCV(GaussianMixture(), gmm_params)\n",
    "gmm_grid.fit(scaled_data)\n",
    "\n",
    "# Get the best parameters for K-means clustering and GMM\n",
    "best_kmeans_params = kmeans_grid.best_params_\n",
    "best_gmm_params = gmm_grid.best_params_\n",
    "\n",
    "print(\"Best parameters for K-means clustering:\", best_kmeans_params)\n",
    "print(\"Best parameters for GMM:\", best_gmm_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-17T21:57:52.904659Z",
     "iopub.status.busy": "2023-06-17T21:57:52.904246Z",
     "iopub.status.idle": "2023-06-17T21:58:01.269015Z",
     "shell.execute_reply": "2023-06-17T21:58:01.267719Z",
     "shell.execute_reply.started": "2023-06-17T21:57:52.904628Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "X = data[['size_of_data', 'virtual_address', 'entropy', 'virtual_size']]\n",
    "y = data['malware']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the parameter grid for KNN\n",
    "knn_params = {\n",
    "    'n_neighbors': [3, 5, 7],  # Vary the number of neighbors\n",
    "    'weights': ['uniform', 'distance'],  # Vary the weighting scheme\n",
    "}\n",
    "\n",
    "# Perform grid search for KNN\n",
    "knn_grid = GridSearchCV(KNeighborsClassifier(), knn_params)\n",
    "knn_grid.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters for KNN\n",
    "best_knn_params = knn_grid.best_params_\n",
    "\n",
    "print(\"Best parameters for KNN:\", best_knn_params)\n",
    "\n",
    "# Train the KNN model with the best parameters\n",
    "best_knn_model = KNeighborsClassifier(**best_knn_params)\n",
    "best_knn_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = best_knn_model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-20T03:16:45.746090Z",
     "iopub.status.busy": "2023-06-20T03:16:45.745687Z",
     "iopub.status.idle": "2023-06-20T03:16:54.285836Z",
     "shell.execute_reply": "2023-06-20T03:16:54.284585Z",
     "shell.execute_reply.started": "2023-06-20T03:16:45.746061Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "X = data[['size_of_data', 'virtual_address', 'entropy', 'virtual_size']]\n",
    "y = data['malware']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the parameter grid for KNN\n",
    "knn_params = {\n",
    "    'n_neighbors': [3, 5, 7],  # Vary the number of neighbors\n",
    "    'weights': ['uniform', 'distance'],  # Vary the weighting scheme\n",
    "}\n",
    "\n",
    "# Perform grid search for KNN\n",
    "knn_grid = GridSearchCV(KNeighborsClassifier(), knn_params)\n",
    "knn_grid.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters for KNN\n",
    "best_knn_params = knn_grid.best_params_\n",
    "\n",
    "print(\"Best parameters for KNN:\", best_knn_params)\n",
    "\n",
    "# Train the KNN model with the best parameters\n",
    "best_knn_model = KNeighborsClassifier(**best_knn_params)\n",
    "best_knn_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = best_knn_model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-17T21:58:55.618427Z",
     "iopub.status.busy": "2023-06-17T21:58:55.617995Z",
     "iopub.status.idle": "2023-06-17T21:59:16.914025Z",
     "shell.execute_reply": "2023-06-17T21:59:16.912901Z",
     "shell.execute_reply.started": "2023-06-17T21:58:55.618395Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Calculate the Silhouette Score\n",
    "silhouette_avg = silhouette_score(scaled_data, kmeans.labels_)\n",
    "print(\"Silhouette Score:\", silhouette_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-17T21:59:38.122136Z",
     "iopub.status.busy": "2023-06-17T21:59:38.121706Z",
     "iopub.status.idle": "2023-06-17T21:59:59.623186Z",
     "shell.execute_reply": "2023-06-17T21:59:59.621992Z",
     "shell.execute_reply.started": "2023-06-17T21:59:38.122104Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Calculate the Silhouette Score\n",
    "silhouette_avg = silhouette_score(scaled_data, gmm.predict(scaled_data))\n",
    "print(\"Silhouette Score:\", silhouette_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Calinski-Harabasz score and Davies-Bouldin score are two additional metrics used to evaluate the quality of clustering in unsupervised learning.\n",
    "\n",
    "The Calinski-Harabasz score, also known as the variance ratio criterion, measures the ratio between the within-cluster dispersion and the between-cluster dispersion. It calculates the compactness of the clusters and the separation between them. The higher the Calinski-Harabasz score, the better the clustering result. The formula for calculating the Calinski-Harabasz score is:\n",
    "\n",
    "Calinski-Harabasz score = (B / W) * (N - k) / (k - 1)\n",
    "\n",
    "where:\n",
    "\n",
    "    B represents the between-cluster dispersion (sum of squared distances between cluster centroids and the overall centroid),\n",
    "    W represents the within-cluster dispersion (sum of squared distances within each cluster),\n",
    "    N is the total number of samples, and\n",
    "    k is the number of clusters.\n",
    "\n",
    "**A higher Calinski-Harabasz score indicates better-defined and well-separated clusters.**\n",
    "\n",
    "The Davies-Bouldin score measures the average similarity between clusters and the dissimilarity between clusters. It evaluates how well the clusters are separated and how compact they are. The lower the Davies-Bouldin score, the better the clustering result. The formula for calculating the Davies-Bouldin score is:\n",
    "\n",
    "Davies-Bouldin score = (1 / N) * sum(max((s(i) + s(j)) / d(i, j)))\n",
    "\n",
    "where:\n",
    "\n",
    "    N is the total number of clusters,\n",
    "    s(i) is the average distance between samples in cluster i and the centroid of cluster i,\n",
    "    s(j) is the average distance between samples in cluster j and the centroid of cluster j, and\n",
    "    d(i, j) is the distance between the centroids of clusters i and j.\n",
    "\n",
    "**A lower Davies-Bouldin score indicates better separation and more compact clusters.**\n",
    "\n",
    "Both the Calinski-Harabasz score and Davies-Bouldin score are used as quantitative measures to compare different clustering algorithms or to evaluate the performance of clustering algorithms with different parameter settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-17T22:00:15.109466Z",
     "iopub.status.busy": "2023-06-17T22:00:15.109052Z",
     "iopub.status.idle": "2023-06-17T22:00:15.172138Z",
     "shell.execute_reply": "2023-06-17T22:00:15.171069Z",
     "shell.execute_reply.started": "2023-06-17T22:00:15.109437Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import calinski_harabasz_score, davies_bouldin_score\n",
    "\n",
    "# Calculate the Calinski-Harabasz Index\n",
    "calinski_harabasz_index = calinski_harabasz_score(scaled_data, gmm.predict(scaled_data))\n",
    "print(\"Calinski-Harabasz Index:\", calinski_harabasz_index)\n",
    "\n",
    "# Calculate the Davies-Bouldin Index\n",
    "davies_bouldin_index = davies_bouldin_score(scaled_data, gmm.predict(scaled_data))\n",
    "print(\"Davies-Bouldin Index:\", davies_bouldin_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-17T22:11:36.209481Z",
     "iopub.status.busy": "2023-06-17T22:11:36.209039Z",
     "iopub.status.idle": "2023-06-17T22:11:43.376252Z",
     "shell.execute_reply": "2023-06-17T22:11:43.375152Z",
     "shell.execute_reply.started": "2023-06-17T22:11:36.209449Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load your data into a DataFrame (assuming 'data' is your DataFrame)\n",
    "\n",
    "# Scatter Plot\n",
    "sns.scatterplot(x='size_of_data', y='entropy', hue='malware', data=data)\n",
    "plt.title('Scatter Plot')\n",
    "plt.show()\n",
    "\n",
    "# Dendrogram\n",
    "\n",
    "\n",
    "# Parallel Coordinates\n",
    "sns.set(style=\"darkgrid\")\n",
    "sns.lineplot(data=data.drop('malware', axis=1))\n",
    "plt.title('Parallel Coordinates')\n",
    "plt.show()\n",
    "\n",
    "# Heatmap\n",
    "heatmap_data = data.drop('malware', axis=1).corr()\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(heatmap_data, annot=True, cmap='coolwarm', square=True)\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying Supervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-20T03:06:48.974547Z",
     "iopub.status.busy": "2023-06-20T03:06:48.974096Z",
     "iopub.status.idle": "2023-06-20T03:06:50.168360Z",
     "shell.execute_reply": "2023-06-20T03:06:50.166787Z",
     "shell.execute_reply.started": "2023-06-20T03:06:48.974512Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Read the data from CSV file\n",
    "data = pd.read_csv('/kaggle/input/malware-analysis-datasets-pe-section-headers/pe_section_headers.csv')\n",
    "data = data.drop('hash', axis=1)\n",
    "# Prepare the features (X) and target variable (y)\n",
    "X = data[['size_of_data', 'virtual_address', 'entropy', 'virtual_size']]\n",
    "y = data['malware']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Logistic Regression model\n",
    "logistic_model = LogisticRegression()\n",
    "\n",
    "# Perform cross-validation and calculate the ROC score\n",
    "roc_scores = cross_val_score(logistic_model, X_train, y_train, cv=5, scoring='roc_auc')\n",
    "roc_score_mean = roc_scores.mean()\n",
    "\n",
    "# Fit the model on the entire training set\n",
    "logistic_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = logistic_model.predict(X_test)\n",
    "\n",
    "# Calculate the ROC score on the test set\n",
    "roc_score_test = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "# Display the confusion matrix\n",
    "confusion_mat = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_mat)\n",
    "\n",
    "# Print the ROC score\n",
    "print(\"Cross-Validation ROC Score:\", roc_score_mean)\n",
    "print(\"Test Set ROC Score:\", roc_score_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-20T03:08:52.323643Z",
     "iopub.status.busy": "2023-06-20T03:08:52.323231Z",
     "iopub.status.idle": "2023-06-20T03:08:53.220616Z",
     "shell.execute_reply": "2023-06-20T03:08:53.219490Z",
     "shell.execute_reply.started": "2023-06-20T03:08:52.323600Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "data = pd.read_csv('/kaggle/input/malware-analysis-datasets-pe-section-headers/pe_section_headers.csv')\n",
    "\n",
    "# Drop the 'hash' column\n",
    "data = data.drop('hash', axis=1)\n",
    "\n",
    "# Split the data into features (X) and target variable (y)\n",
    "X = data.drop('malware', axis=1)\n",
    "y = data['malware']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Construct the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),  # Scale the features\n",
    "    ('logreg', LogisticRegression())  # Logistic Regression model\n",
    "])\n",
    "\n",
    "# Perform cross-validation and calculate ROC score\n",
    "roc_scores = cross_val_score(pipeline, X_train, y_train, cv=5, scoring='roc_auc')\n",
    "\n",
    "# Fit the model on the training data\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Calculate the ROC score on the test set\n",
    "roc_score_test = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "confusion_mat = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Print the results\n",
    "print(\"Cross-Validation ROC Score:\", roc_scores.mean())\n",
    "print(\"Test Set ROC Score:\", roc_score_test)\n",
    "print(\"Confusion Matrix:\\n\", confusion_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-20T03:10:17.624777Z",
     "iopub.status.busy": "2023-06-20T03:10:17.624361Z",
     "iopub.status.idle": "2023-06-20T03:10:18.396216Z",
     "shell.execute_reply": "2023-06-20T03:10:18.394984Z",
     "shell.execute_reply.started": "2023-06-20T03:10:17.624747Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Read the data\n",
    "data = pd.read_csv('/kaggle/input/malware-analysis-datasets-pe-section-headers/pe_section_headers.csv')\n",
    "\n",
    "# Drop the 'hash' column\n",
    "data = data.drop('hash', axis=1)\n",
    "\n",
    "# Split the data into features (X) and target variable (y)\n",
    "X = data.drop('malware', axis=1)\n",
    "y = data['malware']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Create and fit the KNN classifier\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = knn.predict(X_test_scaled)\n",
    "\n",
    "# Calculate the accuracy score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "confusion_mat = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Print the results\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Confusion Matrix:\\n\", confusion_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supervised Learning:\n",
    "In supervised learning, the models are trained on labeled data, where the target variable (or class labels) is known. The objective is to learn a mapping between the input features and the corresponding target labels. The model is optimized to minimize the prediction error by comparing its output with the known labels during training.\n",
    "\n",
    "The reason unsupervised accuracy is generally lower than supervised accuracy is due to the inherent differences in the nature and objectives of the two approaches. However we see here that logistic regression is very bad but KNN absolutely did wonderfully.\n",
    "Here is the reason:\n",
    "\n",
    "    KNN is a non-parametric algorithm and can capture non-linear relationships between features and the target variable. If the relationship between the features and the target variable is non-linear, KNN may be more suitable than logistic regression, which assumes a linear relationship by default.\n",
    "\n",
    "    KNN considers all features equally important when making predictions. If there are certain features in the dataset that have high relevance to the target variable, KNN can leverage this information effectively. Logistic regression, on the other hand, relies on the assumption that all features contribute linearly to the prediction.\n",
    "\n",
    "    KNN does not make any assumptions about the underlying data distribution, while logistic regression assumes a specific distribution (e.g., logistic distribution). If the data deviates from the assumed distribution, logistic regression may yield suboptimal results.\n",
    "\n",
    "\n",
    "\n",
    "Unsupervised Learning:\n",
    "In unsupervised learning, the models are trained on unlabeled data, where the target variable or class labels are not provided. The objective is to find meaningful patterns, structures, or relationships in the data without any prior knowledge of the class labels. Unsupervised learning algorithms focus on exploring the inherent structure or clustering in the data.\n",
    "\n",
    "Since unsupervised learning does not have the benefit of labeled data, it is challenging to evaluate the model's accuracy directly. Instead, unsupervised learning algorithms aim to identify patterns or structures in the data that may not be explicitly labeled or known beforehand.\n",
    "\n",
    "Evaluation in Unsupervised Learning:\n",
    "In unsupervised learning, evaluation metrics are used to assess the quality of the model's performance based on its ability to discover meaningful patterns or clusters in the data. Evaluation metrics such as Silhouette Score, Calinski-Harabasz Index, and Davies-Bouldin Index provide insights into the quality and separation of clusters or the overall structure of the data. These metrics measure the coherence, compactness, and separability of the discovered clusters.\n",
    "\n",
    "It's important to note that the low unsupervised accuracy does not necessarily mean the model is performing poorly. Unsupervised learning focuses on unsupervised tasks like clustering, dimensionality reduction, or anomaly detection, where the primary goal is to uncover hidden structures or relationships within the data. The effectiveness of unsupervised learning is often assessed based on its ability to provide useful insights, generate hypotheses, or aid in data exploration, rather than achieving high accuracy comparable to supervised learning tasks.\n",
    "\n",
    "In summary, supervised learning benefits from labeled data for accurate prediction, while unsupervised learning focuses on discovering patterns or structures without explicit class labels. The evaluation in unsupervised learning relies on metrics that measure the quality of discovered patterns, and the low accuracy is inherent to the different goals and evaluation approaches of unsupervised learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a final step in this project let's talk about what did work out well and where things could be improved.\n",
    "\n",
    "#  Learnings and Takeaways\n",
    "\n",
    "Running through a machine learning project from start to finish really shows how crucial the preparation steps like data cleaning and preparation are for model building. \n",
    "There can be also complex and hidden relationships between the data and the different models (and parameters) that make certain model improvement techniques not work out as we have seen with the feature importance. \n",
    "\n",
    "From this we learned if we have proper datset, we can train the machines to recognize any pattern and employ it to recognize patterns. Here using my model we can use it devices like:\n",
    "\n",
    "1. Firewall\n",
    "2. IPS/IDS\n",
    "3. EMail Scanning devices\n",
    "\n",
    "\n",
    "#  What did not work\n",
    "\n",
    "Since there were not that many features we never needed feature selection. There was another datset I got in Kaggle where there were 1007 features that had PE headers. So for that datset we can use \n",
    "Initially I thought that all the Supervised algorithm would work better than Unsupervised one, but that wasn't the case. We could have dropped few features and then I should have tried that, but that is for another project.\n",
    "\n",
    "\n",
    "#  Possible Improvements\n",
    "\n",
    "If I had more features in the data like number of bytes transferred  or PE info then the model might be more effective.\n",
    "Also I would want to try deep learning on this to see how good that turns out to be,as my bet is that it wouldn't. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "1. https://towardsdatascience.com/clustering-out-of-the-black-box-5e8285220717\n",
    "\n",
    "2. https://towardsdatascience.com/gaussian-mixture-modelling-gmm-833c88587c7f\n",
    "\n",
    "3. https://www.cs.cmu.edu/~guestrin/Class/10701-S07/Slides/clustering.pdf\n",
    "\n",
    "4. https://www.linkedin.com/pulse/pe-malware-static-analysis-ala-a-amarin/\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
